{"cells":[{"cell_type":"markdown","id":"juvenile-enforcement","metadata":{"id":"juvenile-enforcement"},"source":["# Week 1: Explore the BBC News archive\n","\n","Welcome! In this assignment you will be working with a variation of the [BBC News Classification Dataset](https://www.kaggle.com/c/learn-ai-bbc/overview), which contains 2225 examples of news articles with their respective categories (labels).\n","\n","Let's get started!"]},{"cell_type":"code","execution_count":3,"id":"combined-brooklyn","metadata":{"id":"combined-brooklyn","tags":["graded"],"executionInfo":{"status":"ok","timestamp":1714318533983,"user_tz":-120,"elapsed":277,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"outputs":[],"source":["import csv\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","source":[" ! pip install -q kaggle"],"metadata":{"id":"YNrLlaTAWOHA","executionInfo":{"status":"ok","timestamp":1714318549995,"user_tz":-120,"elapsed":7903,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"id":"YNrLlaTAWOHA","execution_count":4,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","\n","files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"6-mpDH0EWTZ0","executionInfo":{"status":"ok","timestamp":1714318596968,"user_tz":-120,"elapsed":33535,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"960ea36c-fa0b-48d4-c422-d3f270b4ddba"},"id":"6-mpDH0EWTZ0","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-626a01ec-8056-4e5e-8c70-de32f02bea26\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-626a01ec-8056-4e5e-8c70-de32f02bea26\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kaggle.json': b'{\"username\":\"victorgruiz\",\"key\":\"bc99abb3bda1b5be7cc01e9bf1c40d09\"}'}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["! mkdir ~/.kaggle"],"metadata":{"id":"3eLlUvpSWeiE","executionInfo":{"status":"ok","timestamp":1714318609336,"user_tz":-120,"elapsed":274,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"id":"3eLlUvpSWeiE","execution_count":6,"outputs":[]},{"cell_type":"code","source":["! cp kaggle.json ~/.kaggle/"],"metadata":{"id":"kdeOUnwgWe5F","executionInfo":{"status":"ok","timestamp":1714318617646,"user_tz":-120,"elapsed":251,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"id":"kdeOUnwgWe5F","execution_count":7,"outputs":[]},{"cell_type":"code","source":["! chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"7NQlLnlgWii7","executionInfo":{"status":"ok","timestamp":1714318626468,"user_tz":-120,"elapsed":261,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"id":"7NQlLnlgWii7","execution_count":8,"outputs":[]},{"cell_type":"code","source":["! kaggle competitions download -c learn-ai-bbc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MCD_FTvRWksi","executionInfo":{"status":"ok","timestamp":1714318817482,"user_tz":-120,"elapsed":1348,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"3051b746-5232-486b-a753-22050ea0bea2"},"id":"MCD_FTvRWksi","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading learn-ai-bbc.zip to /content\n","\r  0% 0.00/1.85M [00:00<?, ?B/s]\n","\r100% 1.85M/1.85M [00:00<00:00, 125MB/s]\n"]}]},{"cell_type":"code","source":["!unzip /content/learn-ai-bbc.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2BVltQ3RXGao","executionInfo":{"status":"ok","timestamp":1714318892944,"user_tz":-120,"elapsed":10,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"84c2536a-e993-4bfe-ea64-27a52b0ba576"},"id":"2BVltQ3RXGao","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/learn-ai-bbc.zip\n","  inflating: BBC News Sample Solution.csv  \n","  inflating: BBC News Test.csv       \n","  inflating: BBC News Train.csv      \n"]}]},{"cell_type":"code","source":["! mkdir -p ./data"],"metadata":{"id":"efUipT0eXp4u","executionInfo":{"status":"ok","timestamp":1714318930609,"user_tz":-120,"elapsed":258,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"id":"efUipT0eXp4u","execution_count":12,"outputs":[]},{"cell_type":"code","source":["! mv /content/BBC* ./data"],"metadata":{"id":"78qu1ofvXu9p","executionInfo":{"status":"ok","timestamp":1714318957197,"user_tz":-120,"elapsed":232,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"id":"78qu1ofvXu9p","execution_count":13,"outputs":[]},{"cell_type":"code","source":["!ls ./data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1C6BZ7tyX1cv","executionInfo":{"status":"ok","timestamp":1714319020014,"user_tz":-120,"elapsed":248,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"e63e7b63-6e31-4a02-ef8a-4ec43c636e24"},"id":"1C6BZ7tyX1cv","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["'BBC News Sample Solution.csv'\t'BBC News Test.csv'  'BBC News Train.csv'\n"]}]},{"cell_type":"code","source":["!mv ./data/* /content/"],"metadata":{"id":"4d4FPQMZYEwb","executionInfo":{"status":"ok","timestamp":1714319056144,"user_tz":-120,"elapsed":256,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"id":"4d4FPQMZYEwb","execution_count":16,"outputs":[]},{"cell_type":"markdown","id":"dependent-power","metadata":{"id":"dependent-power"},"source":["Begin by looking at the structure of the csv that contains the data:"]},{"cell_type":"code","execution_count":18,"id":"finite-panic","metadata":{"tags":["graded"],"colab":{"base_uri":"https://localhost:8080/"},"id":"finite-panic","executionInfo":{"status":"ok","timestamp":1714319121489,"user_tz":-120,"elapsed":3,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"2a622d0c-21bc-469c-ff4f-61f15eb43dc6"},"outputs":[{"output_type":"stream","name":"stdout","text":["First line (header) looks like this:\n","\n","ArticleId,Text,Category\n","\n","Each data point looks like this:\n","\n","1833,worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.,business\n","\n"]}],"source":["with open(\"/content/bbc-text.csv\", 'r') as csvfile:\n","    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n","    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")"]},{"cell_type":"markdown","id":"aggregate-calvin","metadata":{"id":"aggregate-calvin"},"source":["As you can see, each data point is composed of the category of the news article followed by a comma and then the actual text of the article."]},{"cell_type":"markdown","id":"rocky-credit","metadata":{"id":"rocky-credit"},"source":["## Removing Stopwords\n","\n","One important step when working with text data is to remove the **stopwords** from it. These are the most common words in the language and they rarely provide useful information for the classification process.\n","\n","Complete the `remove_stopwords` below. This function should receive a string and return another string that excludes all of the stopwords provided. You only need to account for whitespace as the separation mechanism between words in the sentence."]},{"cell_type":"code","execution_count":32,"id":"permanent-privilege","metadata":{"tags":["graded"],"id":"permanent-privilege","executionInfo":{"status":"ok","timestamp":1714320085184,"user_tz":-120,"elapsed":3,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"outputs":[],"source":["# GRADED FUNCTION: remove_stopwords\n","def remove_stopwords(sentence):\n","    \"\"\"\n","    Removes a list of stopwords\n","\n","    Args:\n","        sentence (string): sentence to remove the stopwords from\n","\n","    Returns:\n","        sentence (string): lowercase sentence without the stopwords\n","    \"\"\"\n","    # List of stopwords\n","    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n","\n","    # Sentence converted to lowercase-only\n","    sentence = sentence.lower()\n","\n","    ### START CODE HERE\n","    sentence =  ' '.join([word for word in sentence.split(' ') if word not in stopwords])\n","    ### END CODE HERE\n","    return sentence"]},{"cell_type":"code","execution_count":33,"id":"northern-third","metadata":{"tags":["graded"],"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"northern-third","executionInfo":{"status":"ok","timestamp":1714320087662,"user_tz":-120,"elapsed":253,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"0d4d270e-2f17-4dad-8657-52d12359aa82"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'go store get snack'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}],"source":["# Test your function\n","remove_stopwords(\"I am about to go to the store and get any snack\")"]},{"cell_type":"markdown","id":"pressed-excellence","metadata":{"id":"pressed-excellence"},"source":["***Expected Output:***\n","```\n","'go store get snack'\n","\n","```"]},{"cell_type":"code","source":["stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n","text = \"I am about to go to the store and get any snack\"\n","newtext = []\n","for word in text.lower().split(' '):\n","  if word not in stopwords:\n","    newtext.append(word)\n","' '.join(newtext)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"N7gBvlpJZSMd","executionInfo":{"status":"ok","timestamp":1714320111664,"user_tz":-120,"elapsed":492,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"6b5e692f-5239-4eaf-e979-a6cd4301e528"},"id":"N7gBvlpJZSMd","execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'go store get snack'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["text = \"I am about to go to the store and get any snack\"\n","' '.join([word for word in text.lower().split(' ') if word not in stopwords])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ogmO-giJZu0F","executionInfo":{"status":"ok","timestamp":1714320115422,"user_tz":-120,"elapsed":11,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"f81ed2e2-6464-40fc-9f2b-2553ed1c70a3"},"id":"ogmO-giJZu0F","execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'go store get snack'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["with open(\"/content/bbc-text.csv\", 'r') as csvfile:\n","    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n","    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")\n","    newline = csvfile.readline()\n","newline.split(',')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-X3ITC3c7RD","executionInfo":{"status":"ok","timestamp":1714320804301,"user_tz":-120,"elapsed":251,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"2118f403-6cb4-4bb6-b27a-b865f9753f6c"},"id":"Q-X3ITC3c7RD","execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["First line (header) looks like this:\n","\n","ArticleId,Text,Category\n","\n","Each data point looks like this:\n","\n","1833,worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.,business\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['154',\n"," 'german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy.  munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months. the study found that the outlook in both the manufacturing and retail sectors had worsened. observers had been hoping that a more confident business sector would signal that economic activity was picking up.   we re surprised that the ifo index has taken such a knock   said dz bank economist bernd weidensteiner.  the main reason is probably that the domestic economy is still weak  particularly in the retail trade.  economy and labour minister wolfgang clement called the dip in february s ifo confidence figure  a very mild decline . he said that despite the retreat  the index remained at a relatively high level and that he expected  a modest economic upswing  to continue.  germany s economy grew 1.6% last year after shrinking in 2003. however  the economy contracted by 0.2% during the last three months of 2004  mainly due to the reluctance of consumers to spend. latest indications are that growth is still proving elusive and ifo president hans-werner sinn said any improvement in german domestic demand was sluggish. exports had kept things going during the first half of 2004  but demand for exports was then hit as the value of the euro hit record levels making german products less competitive overseas. on top of that  the unemployment rate has been stuck at close to 10% and manufacturing firms  including daimlerchrysler  siemens and volkswagen  have been negotiating with unions over cost cutting measures. analysts said that the ifo figures and germany s continuing problems may delay an interest rate rise by the european central bank. eurozone interest rates are at 2%  but comments from senior officials have recently focused on the threat of inflation  prompting fears that interest rates may rise.',\n"," 'business\\n']"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["with open(\"/content/bbc-text.csv\", newline='') as csvfile:\n","  reader = csv.reader(csvfile, delimiter=',')\n","  linecount = 0\n","  sentences = []\n","  labels = []\n","  next(reader)\n","  for row in reader:\n","    if linecount < 6:\n","      sentences.append(remove_stopwords(row[1]))\n","      labels.append(row[2])\n","    else:\n","      break\n","    linecount += 1\n"],"metadata":{"id":"7FTBYXq5dLLh","executionInfo":{"status":"ok","timestamp":1714322587971,"user_tz":-120,"elapsed":242,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"id":"7FTBYXq5dLLh","execution_count":79,"outputs":[]},{"cell_type":"code","source":["labels[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"pRffait9jiM7","executionInfo":{"status":"ok","timestamp":1714322614804,"user_tz":-120,"elapsed":19,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"0a9ee34e-a591-4011-89dd-dbfd1d910e0a"},"id":"pRffait9jiM7","execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'business'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":82}]},{"cell_type":"markdown","id":"animal-photography","metadata":{"id":"animal-photography"},"source":["## Reading the raw data\n","\n","Now you need to read the data from the csv file. To do so, complete the `parse_data_from_file` function.\n","\n","A couple of things to note:\n","- You should omit the first line as it contains the headers and not data points.\n","- There is no need to save the data points as numpy arrays, regular lists is fine.\n","- To read from csv files use [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader) by passing the appropriate arguments.\n","- `csv.reader` returns an iterable that returns each row in every iteration. So the label can be accessed via row[0] and the text via row[1].\n","- Use the `remove_stopwords` function in each sentence."]},{"cell_type":"code","execution_count":85,"id":"monthly-beginning","metadata":{"tags":["graded"],"id":"monthly-beginning","executionInfo":{"status":"ok","timestamp":1714322698163,"user_tz":-120,"elapsed":71,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"outputs":[],"source":["# GRADED FUNCTION: parse_data_from_file\n","def parse_data_from_file(filename):\n","    \"\"\"\n","    Extracts sentences and labels from a CSV file\n","\n","    Args:\n","        filename (string): path to the CSV file\n","\n","    Returns:\n","        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n","    \"\"\"\n","    sentences = []\n","    labels = []\n","    with open(filename, 'r') as csvfile:\n","        ### START CODE HERE\n","        reader = csv.reader(csvfile, delimiter=',')\n","        next(reader)\n","        for row in reader:\n","            sentences.append(remove_stopwords(row[1]))\n","            labels.append(row[2])\n","        ### END CODE HERE\n","    return sentences, labels"]},{"cell_type":"code","execution_count":86,"id":"listed-saturn","metadata":{"tags":["graded"],"colab":{"base_uri":"https://localhost:8080/"},"id":"listed-saturn","executionInfo":{"status":"ok","timestamp":1714322702216,"user_tz":-120,"elapsed":1700,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"0789140a-9000-4520-8707-b0cd67d9d0b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["ORIGINAL DATASET:\n","\n","There are 1490 sentences in the dataset.\n","\n","First sentence has 203 words (after removing stopwords).\n","\n","There are 1490 labels in the dataset.\n","\n","The first 5 labels are ['business', 'business', 'business', 'tech', 'business']\n","\n","\n"]}],"source":["# Test your function\n","\n","# With original dataset\n","sentences, labels = parse_data_from_file(\"/content/bbc-text.csv\")\n","\n","print(\"ORIGINAL DATASET:\\n\")\n","print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n","print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n","print(f\"There are {len(labels)} labels in the dataset.\\n\")\n","print(f\"The first 5 labels are {labels[:5]}\\n\\n\")\n","\n","# # With a miniature version of the dataset that contains only first 5 rows\n","# mini_sentences, mini_labels = parse_data_from_file(\"./data/bbc-text-minimal.csv\")\n","\n","# print(\"MINIATURE DATASET:\\n\")\n","# print(f\"There are {len(mini_sentences)} sentences in the miniature dataset.\\n\")\n","# print(f\"First sentence has {len(mini_sentences[0].split())} words (after removing stopwords).\\n\")\n","# print(f\"There are {len(mini_labels)} labels in the miniature dataset.\\n\")\n","# print(f\"The first 5 labels are {mini_labels[:5]}\")"]},{"cell_type":"markdown","id":"favorite-shanghai","metadata":{"id":"favorite-shanghai"},"source":["***Expected Output:***\n","```\n","ORIGINAL DATASET:\n","\n","There are 2225 sentences in the dataset.\n","\n","First sentence has 436 words (after removing stopwords).\n","\n","There are 2225 labels in the dataset.\n","\n","The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n","\n","\n","MINIATURE DATASET:\n","\n","There are 5 sentences in the miniature dataset.\n","\n","First sentence has 436 words (after removing stopwords).\n","\n","There are 5 labels in the miniature dataset.\n","\n","The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n","\n","```"]},{"cell_type":"markdown","id":"narrative-illness","metadata":{"id":"narrative-illness"},"source":["_**Note: The test above should take less than 5 seconds to run. If your implementation runs longer than that, the grader might time out when you submit. Try to improve it and if it still takes a while to execute, you can click the text below for some hints.**_\n","\n","<br>\n","\n","<details>\n","    <summary><font size=\"3\" color=\"darkgreen\"><b><u>Click for hints</u></b></font></summary>\n","    \n","    # Read the CSV file and specify the comma character as the delimiter\n","    reader = csv.reader(None, delimiter=None)\n","        \n","        # skip the first line using the next() function (1 line of code)\n","        None\n","    \n","        # start for loop and iterate over each row of the reader\n","        for row in reader:\n","    \n","            # append the first element of the row to the `labels` list (1 line of code)\n","            None\n","    \n","            # use the remove_stopwords() function on the second element of the\n","            # row then append to the `sentences` list (1 to 3 lines of code)\n","            None\n","</details>"]},{"cell_type":"markdown","id":"diverse-basket","metadata":{"id":"diverse-basket"},"source":["## Using the Tokenizer\n","\n","Now it is time to tokenize the sentences of the dataset.\n","\n","Complete the `fit_tokenizer` below.\n","\n","This function should receive the list of sentences as input and return a [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) that has been fitted to those sentences. You should also define the \"Out of Vocabulary\" token as `<OOV>`."]},{"cell_type":"code","execution_count":89,"id":"cultural-virginia","metadata":{"tags":["graded"],"id":"cultural-virginia","executionInfo":{"status":"ok","timestamp":1714323141406,"user_tz":-120,"elapsed":3,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"outputs":[],"source":["# GRADED FUNCTION: fit_tokenizer\n","def fit_tokenizer(sentences):\n","    \"\"\"\n","    Instantiates the Tokenizer class\n","\n","    Args:\n","        sentences (list): lower-cased sentences without stopwords\n","\n","    Returns:\n","        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n","    \"\"\"\n","    ### START CODE HERE\n","    # Instantiate the Tokenizer class by passing in the oov_token argument\n","    tokenizer = Tokenizer(oov_token='<OOV>')\n","    # Fit on the sentences\n","    tokenizer.fit_on_texts(sentences)\n","\n","    ### END CODE HERE\n","    return tokenizer"]},{"cell_type":"code","execution_count":90,"id":"tracked-hostel","metadata":{"tags":["graded"],"colab":{"base_uri":"https://localhost:8080/"},"id":"tracked-hostel","executionInfo":{"status":"ok","timestamp":1714323143860,"user_tz":-120,"elapsed":800,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"a92409cb-16a1-47e5-beea-ed4721df4aca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary contains 24963 words\n","\n","<OOV> token included in vocabulary\n"]}],"source":["tokenizer = fit_tokenizer(sentences)\n","word_index = tokenizer.word_index\n","\n","print(f\"Vocabulary contains {len(word_index)} words\\n\")\n","print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"]},{"cell_type":"markdown","id":"moderate-pollution","metadata":{"id":"moderate-pollution"},"source":["***Expected Output:***\n","```\n","Vocabulary contains 29714 words\n","\n","<OOV> token included in vocabulary\n","\n","```"]},{"cell_type":"code","execution_count":91,"id":"golden-flash","metadata":{"tags":["graded"],"id":"golden-flash","executionInfo":{"status":"ok","timestamp":1714323271836,"user_tz":-120,"elapsed":251,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"outputs":[],"source":["# GRADED FUNCTION: get_padded_sequences\n","def get_padded_sequences(tokenizer, sentences):\n","    \"\"\"\n","    Generates an array of token sequences and pads them to the same length\n","\n","    Args:\n","        tokenizer (object): Tokenizer instance containing the word-index dictionary\n","        sentences (list of string): list of sentences to tokenize and pad\n","\n","    Returns:\n","        padded_sequences (array of int): tokenized sentences padded to the same length\n","    \"\"\"\n","\n","    ### START CODE HERE\n","    # Convert sentences to sequences\n","    sequences = tokenizer.texts_to_sequences(sentences)\n","\n","    # Pad the sequences using the post padding strategy\n","    padded_sequences = pad_sequences(sequences, padding='post')\n","    ### END CODE HERE\n","\n","    return padded_sequences"]},{"cell_type":"code","execution_count":92,"id":"spanish-entrepreneur","metadata":{"tags":["graded"],"colab":{"base_uri":"https://localhost:8080/"},"id":"spanish-entrepreneur","executionInfo":{"status":"ok","timestamp":1714323274865,"user_tz":-120,"elapsed":348,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"67ed292c-36b9-4b97-f6ac-fefc5d8bbb97"},"outputs":[{"output_type":"stream","name":"stdout","text":["First padded sequence looks like this: \n","\n","[1322 1180  592 ...    0    0    0]\n","\n","Numpy array of all sequences has shape: (1490, 1881)\n","\n","This means there are 1490 sequences in total and each one has a size of 1881\n"]}],"source":["padded_sequences = get_padded_sequences(tokenizer, sentences)\n","print(f\"First padded sequence looks like this: \\n\\n{padded_sequences[0]}\\n\")\n","print(f\"Numpy array of all sequences has shape: {padded_sequences.shape}\\n\")\n","print(f\"This means there are {padded_sequences.shape[0]} sequences in total and each one has a size of {padded_sequences.shape[1]}\")"]},{"cell_type":"markdown","id":"wired-brief","metadata":{"id":"wired-brief"},"source":["***Expected Output:***\n","```\n","First padded sequence looks like this:\n","\n","[  96  176 1157 ...    0    0    0]\n","\n","Numpy array of all sequences has shape: (2225, 2438)\n","\n","This means there are 2225 sequences in total and each one has a size of 2438\n","\n","```"]},{"cell_type":"code","execution_count":93,"id":"unknown-optimization","metadata":{"tags":["graded"],"id":"unknown-optimization","executionInfo":{"status":"ok","timestamp":1714323505074,"user_tz":-120,"elapsed":219,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}}},"outputs":[],"source":["# GRADED FUNCTION: tokenize_labels\n","def tokenize_labels(labels):\n","    \"\"\"\n","    Tokenizes the labels\n","\n","    Args:\n","        labels (list of string): labels to tokenize\n","\n","    Returns:\n","        label_sequences, label_word_index (list of string, dictionary): tokenized labels and the word-index\n","    \"\"\"\n","    ### START CODE HERE\n","\n","    # Instantiate the Tokenizer class\n","    # No need to pass additional arguments since you will be tokenizing the labels\n","    label_tokenizer = Tokenizer()\n","\n","    # Fit the tokenizer to the labels\n","    label_tokenizer.fit_on_texts(labels)\n","\n","    # Save the word index\n","    label_word_index = label_tokenizer.word_index\n","\n","    # Save the sequences\n","    label_sequences = label_tokenizer.texts_to_sequences(labels)\n","\n","    ### END CODE HERE\n","\n","    return label_sequences, label_word_index"]},{"cell_type":"code","execution_count":94,"id":"streaming-conviction","metadata":{"tags":["graded"],"colab":{"base_uri":"https://localhost:8080/"},"id":"streaming-conviction","executionInfo":{"status":"ok","timestamp":1714323506726,"user_tz":-120,"elapsed":335,"user":{"displayName":"Victor Ruiz","userId":"12524117647542674544"}},"outputId":"535297b9-1062-4600-e7b7-e2a20f6a8f3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary of labels looks like this {'sport': 1, 'business': 2, 'politics': 3, 'entertainment': 4, 'tech': 5}\n","\n","First ten sequences [[2], [2], [2], [5], [2], [3], [1], [4], [2], [4]]\n","\n"]}],"source":["label_sequences, label_word_index = tokenize_labels(labels)\n","print(f\"Vocabulary of labels looks like this {label_word_index}\\n\")\n","print(f\"First ten sequences {label_sequences[:10]}\\n\")"]},{"cell_type":"markdown","id":"endangered-poultry","metadata":{"id":"endangered-poultry"},"source":["***Expected Output:***\n","```\n","Vocabulary of labels looks like this {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n","\n","First ten sequences [[4], [2], [1], [1], [5], [3], [3], [1], [1], [5]]\n","\n","```"]},{"cell_type":"markdown","id":"cross-chess","metadata":{"id":"cross-chess"},"source":["**Congratulations on finishing this week's assignment!**\n","\n","You have successfully implemented functions to process various text data processing ranging from pre-processing, reading from raw files and tokenizing text.\n","\n","**Keep it up!**"]}],"metadata":{"dlai_version":"1.2.0","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}